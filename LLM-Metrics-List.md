## Metrics List
The following table captures the details of the metrics computed in watsonx.gov for LLMs.

| S.No | Metric ID                     | Metric Group        | Task Type                                                   | Reference Free | Reference Based | Runtime Support | Design Time Support |
|------|-------------------------------|---------------------|-------------------------------------------------------------|----------------|-----------------|------------------|--------------------|
| 1    | accuracy                       |                     | Classification                                               |                |         ✅       |   ✅      |   ✅       |
| 2    | answer_relevance              |                     | RAG                                                          |      ✅           |                 |  ✅        |  ✅        |
| 3    | answer_similarity             |                     | RAG                                                          |                |       ✅           |         |   ✅        |
| 4   | average_abs_odds_difference                   |   |    Classification                                          |                       |    ✅             |   ✅      |   ✅       |
| 5   | average_odds_difference                   |   |  Classification                                           |                       |    ✅             |   ✅      |   ✅       |
| 6    | bleu                           |                     | Text Summarization, QA, Content Generation, RAG              |                |         ✅        |    ✅       |  ✅        |
| 7    | brier_score                   |                     | Classification                                               |                |         ✅         |   ✅      |   ✅       |
| 8   |disparate_impact                   |   | Classification                                       |     ✅                   |                |   ✅      |   ✅       |
| 9   | error_rate_difference                   |   | Classification                                             |                       |    ✅             |   ✅      |   ✅       |
| 10    | exact_match                   |                     | QA, Entity Extraction, RAG                                   |                |       ✅           |    ✅       |   ✅       |
| 11   | faithfulness                  |                     | Text Summarization, RAG                                      |         ✅        |                 |    ✅       |    ✅       |
| 12   | false_discovery_rate_difference                   |   |  Classification                                            |                       |    ✅             |   ✅      |   ✅       |
| 13   | false_negative_rate_difference                   |   |  Classification                                            |                       |    ✅             |   ✅      |   ✅       |
| 14   | false_omission_rate_difference                   |   |  Classification                                            |                       |    ✅             |   ✅      |   ✅       |
| 15   | false_positive_rate_difference                   |   |  Classification                                            |                       |    ✅             |   ✅      |   ✅       |
| 16   | impact_score                   |   |    Classification                                          |     ✅                    |               |   ✅      |   ✅       |
| 17    | hap_input_score               |                     | Text Summarization, Classification, Content Generation, Entity Extraction, QA, RAG | ✅            |          |    ✅       |  ✅        |
| 18    | hap_score                     |                     | Text Summarization, Classification, Content Generation, Entity Extraction, QA, RAG |   ✅             |               |   ✅        |   ✅       |
| 19   | keywords_inclusion            |                     | Text Summarization, QA, RAG                                  |                |      ✅            |         |    ✅       |
| 20   | label_distribution            |                     | Classification                                               |    ✅             |                 |         |    ✅     |
| 21   | matthews_correlation_coefficient|                     | Classification                                               |                |     ✅             |  ✅       | ✅         |
| 22   | meteor                        |                     | Text Summarization, Content Generation                       |                |      ✅            |   ✅      |   ✅      |
| 23   | normalized_f1                 |                     | Text Summarization, Content Generation                       |                |      ✅            |   ✅      |   ✅      |
| 24   | normalized_precision           |                     | Text Summarization, Content Generation                       |                |      ✅            |   ✅      |   ✅      |
| 25   | normalized_recall             |                     | Text Summarization, Content Generation                       |                |       ✅           |   ✅      |   ✅      |
| 26   | pii                           |                     | Text Summarization, Classification, Content Generation, Entity Extraction, QA, RAG |  ✅           |                 |    ✅       |   ✅       |
| 27   | pii_input                     |                     | Text Summarization, Classification, Content Generation, Entity Extraction, QA, RAG |  ✅           |                 |   ✅        |  ✅        |
| 28   | prompt_injection              |                     | Content Generation, QA, RAG, Entity Extraction, Text Summarization, Classification |  ✅            |                 |  yet to be released       |  yet to be released       |
| 29   | question_robustness           |                     | QA, RAG                                                      |     ✅           |                  |         |     ✅      |
| 30   | roc_auc                       |                     | Classification                                               |                |         ✅         |   ✅      |   ✅       |
| 31   | rouge_score                   |                     | Text Summarization, Generation, QA, Entity Extraction, RAG   |                |   ✅               |    ✅       |  ✅        |
| 32   | sari                          |                     | Text Summarization                                           |                |   ✅               |   ✅        |   ✅        |
| 33   | statistical_parity_difference                   |   |  Classification                                            |                  ✅      |                |   ✅      |   ✅       |
| 34   | true_positive_rate_difference                   |   | Classification                                             |                       |    ✅             |   ✅      |   ✅       |
| 35   | unsuccessful_requests          |                     | QA, RAG                                                      |      ✅           |                 |      ✅     |   ✅        |
| 36   | flesch_reading_ease           |   Flesch                  | Text Summarization, Content Generation                       |     ✅            |                |   ✅      |   ✅      |
| 37   | f1_measure                    |   Text Quality                  | Classification                                               |                |     ✅             |  ✅       |   ✅       |
| 38   | precision                     |   Text Quality                  | Classification                                               |                |   ✅               |  ✅       |   ✅       |
| 39   | recall                        |   Text Quality                  | Classification                                               |                |      ✅            |   ✅      |   ✅       |
| 40   | jaccard_similarity            |   Sentence Similarity                  | Text Summarization                                           |                |     ✅             |  ✅       |   ✅      |
| 41   | cosine_similarity             |   Sentence Similarity                  | Text Summarization                                           |                |        ✅          |   ✅      |   ✅      |
| 42   | adversarial_robustness        |   Robustness                  | Content Generation, QA, RAG, Entity Extraction, Text Summarization, Classification |✅     |         |         |    ✅       |
| 43   | abstractness                   | Content Analysis     | Text Summarization, RAG                                      |      ✅           |                 |   ✅        |    ✅       |
| 44   | compression                    | Content Analysis     | Text Summarization                                           |      ✅           |                 |    ✅       |    ✅       |
| 45   | coverage                       | Content Analysis     | Text Summarization, RAG                                      |      ✅           |                 |    ✅       |    ✅       |
| 46   | density                        | Content Analysis     | Text Summarization, RAG                                      |      ✅           |                 |     ✅      |     ✅      |
| 47   | repetitiveness                 | Content Analysis     | Text Summarization                                           |      ✅           |                 |   ✅       |   ✅       |
| 48   | average_precision              | Retrieval Quality    | RAG                                                          |      ✅           |                 |   ✅        |         |
| 49   | context_relevance              | Retrieval Quality    | RAG                                                          |      ✅           |                 |   ✅        |    ✅       |
| 50   | hit_rate                       | Retrieval Quality    | RAG                                                          |     ✅            |                 |    ✅       |    ✅       |
| 51   | ndcg                           | Retrieval Quality    | RAG                                                          |      ✅           |                 |      ✅     |     ✅      |
| 52   | reciprocal_rank                | Retrieval Quality    | RAG                                                          |     ✅            |                 |      ✅     |      ✅     |
| 53   | retrieval_precision            | Retrieval Quality    | RAG                                                          |     ✅          |                 |       ✅    |   ✅       |
| 54   | contains_all                   | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |  ✅               |                 |         |   ✅       |
| 55   | contains_any                   | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |     ✅            |                 |         |   ✅       |
| 56   | contains_email                 | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |    ✅             |                 |         |   ✅       |
| 57   | contains_json                  | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅           |                 |         |   ✅       |
| 58   | contains_link                  | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |       ✅         |                 |         |    ✅      |
| 59   | contains_none                  | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |     ✅            |                 |         |   ✅       |
| 60   | contains_string                | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |     ✅           |                 |         |    ✅      |
| 61   | contains_valid_link            | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅           |                 |         |   ✅       |
| 62   | ends_with                      | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |     ✅            |                 |         |   ✅       |
| 63   | equals_to                      | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |     ✅           |                 |         |    ✅      |
| 64   | fuzzy_match                    | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅          |                 |         |    ✅      |
| 65   | is_email                       | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |     ✅          |                 |         |     ✅     |
| 66   | is_json                        | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅          |                 |         |    ✅      |
| 67   | length_greater_than            | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅          |                 |         |    ✅      |
| 68   | length_less_than               | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅          |                 |         |    ✅      |
| 69   | no_invalid_links               | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅          |                 |         |    ✅      |
| 70   | regex                          | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅          |                 |         |    ✅      |
| 71   | starts_with                    | Content Validation   | Text Summarization, Content Generation, QA, RAG, Entity extraction |      ✅          |                 |        |     ✅     |
| 72   | micro_f1                       | Multi-label metrics  | Entity Extraction                                             |                       |     ✅            |  ✅      |  ✅        |
| 73   | macro_f1                       | Multi-label metrics  | Entity Extraction                                             |                       |     ✅            |   ✅      | ✅         |
| 74   | micro_precision                | Multi-label metrics  | Entity Extraction                                             |                       |     ✅            |   ✅      | ✅         | 
| 75   | micro_recall                   | Multi-label metrics  | Entity Extraction                                             |                       |    ✅             |   ✅      |  ✅        |
| 76   | macro_precision                | Multi-label metrics  | Entity Extraction                                             |                       |    ✅             |   ✅      |  ✅        |
| 77   | macro_recall                   | Multi-label metrics  | Entity Extraction                                             |                       |    ✅             |   ✅      |   ✅       |


## Metrics description
The following table will provide the description of metrics. The order of metrics in this table is same as `Metrics List` table above . 


| S.No | Metric ID                     | Description                                                                                           |
|------|--------------------------------|-------------------------------------------------------------------------------------------------------|
| 1    | accuracy                       |    Accuracy is proportion of correct predictions.                                                                                                    |
| 2    | answer_relevance               |   Answer relevance measures how relevant the answer or generated text is to the question sent to the LLM. This is one of the ways to determine the quality of your model.The answer relevance score is a value between 0 and 1.A value closer to 1 indicates that the answer is more relevant to the given question.A value closer to 0 indicates that the answer is less relevant to the question.                                                                                                                     |
| 3    | answer_similarity              |  Answer similarity measures how similar the answer or generated text is to the ground truth or reference answer as judged by a LLM.The answer similarity score is a value between 0 and 1.A value closer to 1 indicates that the answer is more similar to the reference value.A value closer to 0 indicates that the answer is less similar to the reference value.                                                                                                                                          |
| 4   | average_abs_odds_difference                   | Average absolute odds difference is the average of the absolute difference in false positive rate and true positive rate for the monitored and reference groups. The metric value must be close to 0 for better fairness. 
| 5   | average_odds_difference                   | Average odds difference is the average of difference in false positive rates and true positive rates between monitored groups and reference groups. The metric value must be close to 0 for better fairness.
| 6    | bleu                           |   BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.The metric values are in the range of 0 to 1, higher value is better.                                                                                                    |
| 7    | brier_score                    |   The Brier score measures the mean squared difference between the predicted probability and the target value                                                                                                                                         |
| 8   |disparate_impact                   | Disparate impact is the ratio of percentage of favorable outcomes for a monitored group to that of a reference group. The metrics value is represented as percentage, a value close to 100 indicates better fairness. 
| 9   | error_rate_difference                   | Error rate is the ratio of number of incorrectly predicted transactions to the total transactions. Error rate difference is the difference between error rate for the monitored and reference groups. The metric value must be close to 0 for better fairness. 
| 10    | exact_match                    |    A given predicted string's exact match score is 1 if it is the exact same as its reference string, and is 0 otherwise.                                                                                                                                       |
| 11    | faithfulness                   |    Faithfulness measures how faithful the model output or generated text is to the context sent to the LLM.The faithfulness score is a value between 0 and 1.A value closer to 1 indicates that the output is more faithful or grounded and less hallucinated.A value closer to 0 indicates that the output is less faithful or grounded and more hallucinated.                                                                                                                                       |
| 12   | false_discovery_rate_difference                   | False discovery rate is the ratio of the number of false positive transactions to the number of total transactions. False discovery rate difference is the difference between false discovery rate for the monitored and reference groups. The metric value must be close to 0 for better fairness. 
| 13   | false_negative_rate_difference                   | False negative rate is the ratio of number of incorrectly predicted negative transactions to the total positive transactions. False negative rate difference is the difference between false negative rates for the monitored and reference groups. The metric value must be close to 0 for better fairness. 
| 14   | false_omission_rate_difference                   | False omission rate is the ratio of number of incorrectly predicted negative transactions to the total negative transactions.False omission rate is the difference between false omission rates for the monitored and reference groups. The metric value must be close to 0 for better fairness. 
| 15   | false_positive_rate_difference                   | False positive rate is the ratio of number of incorrectly predicted positive transactions to the total negative transactions. False positive rate difference is the difference between false positive rates for the monitored and reference groups. The metric value must be close to 0 for better fairness. 
| 16   | impact_score                   | Impact ratio measures the selection rate for a category divided by the selection rate of the most selected category. The metric value must be close to 1 for better fairness.                                                                                                     |
| 17  | hap_input_score                |   HAP measures if the provided content contains any Hate, Abuse and Profanity. Uses the HAP related model from Watson NLP to measure this metric. This metric measures if there exists any HAP in the input data.                                                                                                      |
| 18   | hap_score                      |  HAP measures if the provided content contains any Hate, Abuse and Profanity. Uses the HAP related model from Watson NLP to measure this metric. This metric measures if there exists any HAP in the output data.                                                                                                      |
| 19   | keywords_inclusion             | Keyword inclusion metric measures keyword density. It is computed by fetching the keywords from predictions and comparing it with keywords present in reference/ground truth. If prediction includes a larger portion of keywords from ground truth then LLM is considered to be highly efficient.                                                                                                      |
| 20   | label_distribution             |  The label distribution metric returns the fraction of each label represented in the dataset.                                                                                                     |
| 21   | matthews_correlation_coefficient|  The Matthews correlation coefficient is used as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes.                                                                                                                                             |
| 22   | meteor                         |   METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a machine translation evaluation metric, which is calculated based on the harmonic mean of precision and recall, with recall weighted more than precision.The metric values are in the range of 0 to 1, higher value is better.                                                                                                                                                |
| 23   | normalized_f1                  |      The normalized F1 metric value in a given evaluation.                                                                                                 |
| 24   | normalized_precision           |     The normalized precision metric value in a given evaluation.                                                                                                  |
| 25   | normalized_recall              |     The normalized recall metric value in a given evaluation.                                                                                                  |
| 26   | pii                            |     PII measures if the provided content contains any Personally Identifiable Information. This metric measures if there exists any PII in the output data.                                                                                                                                       |
| 27   | pii_input                      |   PII measures if the provided content contains any Personally Identifiable Information. This metric measures if there exists any PII in the input data.                                                                                                                                          |
| 28   | prompt_injection               | Attacks that trick the system by combining harmful user input with the trusted prompt created by the  developer.    |
| 29   | question_robustness            | Metric for evaluating spelling robustness, grammar robustness in the questions. |
| 30   | roc_auc                        |   Area under recall and false positive rate curve.                                                                                                    |
| 31   | rouge_score                    |   ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing.Generative text quality monitor calculates rouge1, rouge2, rougeL, and rougeLSum to compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.The metric values are in the range of 0 to 1, higher value is better.                                                                                                                                          |
| 32   | sari                           |   SARI (system output against references and against the input sentence) is a metric used for evaluating automatic text simplification systems. The metric compares the predicted simplified sentences against the reference and the source sentences. It explicitly measures the goodness of words that are added, deleted and kept by the system.The range of values for the SARI score is between 0 and 100 -- the higher the value, the better the performance of the model being evaluated, with a SARI of 100 being a perfect score.                                                                                                                                            |
| 33   | statistical_parity_difference                   | Statistical parity difference is the difference between the favorable outcomes for monitored groups to those of reference groups. The metric value must be close to 0 for better fairness.
| 34   | true_positive_rate_difference                   | True positive rate is the ratio of number of correctly predicted positive transactions to the total positive transactions. True positive date difference is the difference between true positive rates for the monitored and reference groups. The metric value must be close to 0 for better fairness. 
| 35   | unsuccessful_requests          |   The unsuccessful requests metric measures the ratio of questions answered unsuccessfully out of the total number of questions.The unsuccessful requests score is a value between 0 and 1.A value closer to 0 indicates that the model is successfully answering the questions.A value closer to 1 indicates the model is not able to answer the questions.                                                                                                    |
| 36  | flesch_reading_ease            |    The Readability score determines readability, complexity, and grade level of the model's output.A score between 90-100 means Very Easy to Read,80-89 means Easy to Read,70-79 means Fairly Easy to Read,60-69 means Standard to Read,50-59 means Fairly Difficult to Read,30-49 means Difficult to Read,0-29 means Very Confusing to Read                                                                                                                                               |
| 37   | f1_measure                     |    Harmonic mean of precision and recall                                                                                                    |
| 38   | precision                      |     Proportion of correct predictions in predictions of positive class.                                                                                                                                         |
| 39   | recall                         |    Proportion of correct predictions in positive class.                                                                                                   |
| 40   | jaccard_similarity             |  Jaccard similarity measures the similarity between sets of text data, which is used to quantify the similarity between two sets of words or tokens in text.                                                                                                                                              |
| 41   | cosine_similarity              | Cosine similarity is a fundamental similarity metric widely used in various fields, including NLP, large language models, information retrieval, text summarization, text generation.                                                                                                                    |
| 42   | adversarial_robustness        |    This metric checks how well the prompt template can resist jailbreak and prompt injection attacks.                                  |
| 43   | abstractness                   |  Abstractness measures the ratio of n-grams in the generated text output that do not appear in the source content of the foundation model.The abstractness score is a value between 0 and 1. Higher scores indicate high abstractness in the generated text output.                                                                                                     |
| 44   | compression                    |  Compression measures how much shorter the summary is when compared to the input text. It calculates the ratio between the number of words in the original text and the number of words in the foundation model output. The compression score is a value above 0. Higher scores indicate that the summary is more concise when compared to the original text.                                                                                                     |
| 45   | coverage                       |   Coverage measures the extent that the foundation model output is generated from the model input by calculating the percentage of output text that is also in the input. The coverage score is a value between 0 and 1. A higher score close to 1 indicates that higher percentage of output words are within the input text.                                                                                                    |
| 46   | density                        |   Density measures how extractive the summary in the foundation model output is from the model input by calculating the average of extractive fragments that closely resemble verbatim extractions from the original text. The density score is a value above 0. Lower scores indicate that the model output is more abstractive and on average the extractive fragments do not closely resemble verbatim extractions from the original text.                                                                                                    |
| 47   | repetitiveness                 |  Repetitiveness measures the percentage of n-grams that repeat in the foundation model output by calculating the number of repeated n-grams and the total number of n-grams in the model output.The repetitiveness score is a value between 0 and 1.                                                                                                    |
| 48   | average_precision              |  Average Precision evaluates whether all the relevant contexts are ranked higher or not. It is the mean of the precision scores of relevant contexts. The average precision is a value between 0 and 1.A value of 1 indicates that all the relevant contexts are ranked higher.A value of 0 indicates that none of the retrieved contexts are relevant.                                                                                                     |
| 49   | context_relevance              | Context relevance assesses the degree to which the retrieved context is relevant to the question sent to the LLM. This is one of the ways to determine the quality of your retrieval system.The context relevance score is a value between 0 and 1.<br></br>A value closer to 1 indicates that the context is more relevant to your question in the prompt.A value closer to 0 indicates that the context is less relevant to your question in the prompt.                                                                                                      |
| 50   | hit_rate                       |  Hit Rate measures whether there is atleast one relevant context among the retrieved contexts. The hit rate value is either 0 or 1. A value of 1 indicates that there is at least one relevant context. A value of 0 indicates that there is no relevant context in the retrieved contexts.                                                                                                     |
| 51   | ndcg                           |  Normalized Discounted Cumulative Gain or NDCG measures the ranking quality of the retrieved contexts.The ndcg is a value between 0 and 1.A value of 1 indicates that the retrieved contexts are ranked in the correct order.                                                                                                     |
| 52   | reciprocal_rank                | Reciprocal rank is the reciprocal of the rank of the first relevant context.The retrieval reciprocal rank is a value between 0 and 1.A value of 1 indicates that the first relevant context is at first position.A value of 0 indicates that none of the relevant contexts are retrieved.                                                                                                       |
| 53   | retrieval_precision            |  Retrieval Precision measures the quantity of relevant contexts from the total contexts retrieved. The retrieval precision is a value between 0 and 1.A value of 1 indicates that all the retrieved contexts are relevant.A value of 0 indicates that none of the retrieved contexts are relevant.                                                                                                     |
| 54   | contains_all                   | The Contains All metric evaluates whether the rows in the prediction include all specified keywords, with thresholds set between 0 and 1. A value of 1 indicates that all specified keywords are present in the rows, while a value of 0 signifies that they are not.                                                                                                      |
| 55   | contains_any                   | The Contains Any metric evaluates whether the rows in the prediction include any of the specified keywords, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain at least one of the specified keywords, while a value of 0 signifies that they do not contain any of the keywords.                                                                                                      |
| 56   | contains_email                 |  The Contains Email metric evaluates whether each row in the prediction includes email addresses, with thresholds set between 0 (does not contain emails) and 1 (contains emails).                                                                                                      |
| 57   | contains_json                  | The Contains_JSON metric evaluates whether the rows in the prediction include JSON syntax, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain JSON syntax, while a value of 0 signifies that they do not.                                                                                                      |
| 58   | contains_link                  | The Contains Link metric evaluates whether the rows in the prediction include any links, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain links, while a value of 0 signifies that they do not.                                                                                                      |
| 59   | contains_none                  | The Contains None metric assesses whether the rows in the prediction do not include any of the specified keywords, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain none of the specified keywords, while a value of 0 signifies that they do contain at least one of the keywords.                                                                                                      |
| 60   | contains_string                | The Contains String metric evaluates whether each row in the prediction includes the specified string, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain the specified string, while a value of 0 signifies that they do not.                                                                                                      |
| 61   | contains_valid_link            | The Contains Valid Link metric evaluates whether the rows in the prediction include valid links, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain valid links, while a value of 0 signifies that they do not.                                                                                                      |
| 62   | ends_with                      | The Ends With metric evaluates whether the rows in the prediction conclude with a specified substring, with thresholds set between 0 and 1. A value of 1 indicates that the rows end with the specified substring, while a value of 0 signifies that they do not.                                                                                                      |
| 63   | equals_to                      |  The Equals To metric assesses whether the rows in the prediction are equal to a specified substring, with thresholds set between 0 and 1. A value of 1 indicates that the row is equal to the specified substring, while a value of 0 signifies that it is not.                                                                                                     |
| 64   | fuzzy_match                    | The Fuzzy Match metric assesses whether the prediction fuzzy matches the specified keyword, with thresholds set between 0 and 1. A value of 1 indicates that the prediction fuzzy matches the keyword, while a value of 0 signifies that it does not.                                                                                                      |
| 65   | is_email                       | The Is Email metric assesses whether the rows in the prediction contain valid email addresses, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain valid emails, while a value of 0 signifies that they do not.                                                                                                      |
| 66   | is_json                        | The Is JSON metric assesses whether the rows in the prediction contain valid JSON syntax, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain valid JSON syntax, while a value of 0 signifies that they do not.                                                                                                      |
| 67   | length_greater_than            | The Length Greater Than metric assesses whether the length of each row in the prediction exceeds a specified maximum value, with thresholds ranging from 0 to 1. A value of 1 indicates that the row length in the prediction is greater than the specified value, while a value of 0 indicates that it is not.                                                                                                      |
| 68   | length_less_than               | The Length Less Than metric evaluates whether the length of each row in the prediction is below a specified maximum value, with thresholds set between 0  and 1.A value of 1 indicates that the row length in the prediction is less than the specified value and 0 indicates that the row length is not less than the specified value.                                                                                                  |
| 69   | no_invalid_links               | The No Invalid Links metric assesses whether the rows in the prediction contain no invalid links, with thresholds set between 0 and 1. A value of 1 indicates that the rows have no invalid links, while a value of 0 signifies that they do contain invalid links.                                                                                                      |
| 70   | regex                          |  The Regex metric assesses whether the rows in the prediction contain the specified regex expression, with thresholds set between 0 and 1. A value of 1 indicates that the rows contain the specified regex expression, while a value of 0 signifies that they do not.                                                                                                     |
| 71   | starts_with                    | The Starts With metric assesses whether the rows in the prediction begin with a specified substring, with thresholds set between 0 and 1. A value of 1 indicates that the rows start with the specified substring, while a value of 0 signifies that they do not.                                                                                                      |
| 72   | micro_f1                       |  It is computed by taking the harmonic mean of precision and recall.                                                                                                     |
| 73   | macro_f1                       |  It is computed by taking the arithmetic mean of all the per-class F1 scores.                                                                                                     |
| 74   | micro_precision                |  It is the ratio of number of correct predictions over all classes to the number of total predictions.                                                                                                     |
| 75   | micro_recall                   |  It is the ratio of number of correct predictions over all classes to the number of true samples.                                                                                                     |
| 76   | macro_precision                |  It is the ratio of number of correct predictions over all classes to the number of total predictions.                                                                                                     |
| 77   | macro_recall                   | It calculates the recall for each individual label or class separately and then takes the average of those recall scores. 

Refer to documentation to get more metric descriptions. Few reference link as under :
- https://dataplatform.cloud.ibm.com/docs/content/wsj/model/wos-monitor-gen-quality.html?context=wx&audience=wdp